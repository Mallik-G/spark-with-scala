# SparkSQL

Everything is moving to datasets and dataframes

## Working with structured data

* Extends RDD to a "DataFrame" object
* DataFrames:
    - Contain row objects
    - Can run SQL queries
    - Has a schema (leading to more efficient storage)
    - Read and write to JSON, Hive, parquet
    - Communicates with JDBC/ODBC, Tableu

## DataSets

What is the diff between ths and dataframe

* Dataframe is really just a dataset of row objects
    - DataSet[Row]
* DataSets can explicitly wrap a given struct or type
    - DataSet[Person] or DataSet[(String, Double)]
    - It knows what it's columns are from the get-go
* Dataframes schema is inferred at runtime, but a dataset can be inferred at compile time
    - Faster detection of errors and better optimization
* RDDs can be converted to DataSets with .toDS()


## Datasets are the new hotness

We learned a lot about RDDs, they are a fundamental, but trend is ddtasets wherever possible

NOt always you have structured data, but many times you can.

For a lot of data analysis, DS will be the datastruct of choice

* DataSets are more efficient
    - Can be serialized very efficiently - even better than kryo
    - Optimal execution plans can be determined at compile time
* Datasets allow for better interoperability
    - MLLib and spark streaming are moving towards using datasets instead of RDDs for their primary API
* Datasets simplify development
    - You can perform most SQL operations on a dataste with one line


## How ot use Spark SQL in Scala

* Spark 2.0 you can create a sparksession object instead of SparkContext 
    - You can get a sparkcontext from this session, and use it to issue sql queries
    - need to shut it down


## Other stuff you can do

* myResult.show ()
* myResult.select("somefield")
* myResult.filter(myResult("column") > 20)
* myResult.groupBy(myResult("ffield")).mean()
* myResult.rdd().map(mapperfunc)


## Shell Access

* SparkSQL exposes JDBC/ODBC serer
* Start it with `sbin/start-thriftserver.sh`
* Listens port 10000 by default
* Connnect using bin/beeline -u jdbc:hive2://localhost:10000
* Viola, you have a SQL shell to SparkSQL
* You can create new tables, or query existing ones that were cached using hiveCtx.cacheTable("tableName")

## User-defined functions

``` scala
import org.apache.spark.sql.functions.udf
val sqare = (x => x*x)
squareDF = df.withColumn("square", square('value'))
```

## Lets' play

* Use our fake social network
* Query it with SQL and then use Datasets without SQL
* Finally, we'll re-do our popular movies example with datasets and see how much simpler it is




   
