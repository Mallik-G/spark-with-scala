# Computing Movie Similarities

Finding similar movies using Spark and the MovieLens data set 

And introducing caching RDDs and persisting them! Something we need if we use RDDs more than once!

## Similar Movies

**Item based collaborative filtering!**

We try to find relationships between items - in this case movies based on customer behaviour

If we see 2 movies that users tend to rate together similarly, then maybe these movies might be similar.

And if one likes a movie, they might also like other "similar" movies that are connected through that behaviour data!

Movielens website has a recommender website!

e-commerce uses it as well!

## Overview

* Find every pair of movie that were watched by the same person
* Measure the similarity of their ratings across all users who watched both
* Sort by movie, then by similarity strength
* (This is just one way to do it)

### Example

* There is a lady that watched starwars 
    - and the empire strkes back
* Now we know the two movies were watched by the same person 
* There was another guy that watched those movies too
* By comparing behaviour of people that watch the same movies, might tell us that they watch similar movies
    - That might tell us that those movies are similar to each other
* But there might be another random guy that just comes in and watches the empire strikes back - so after you see it, they might tell you that given you watched X movie, you might also like Y!!

We like at people watching similar movies and rated them similarly.

It doesn't mean that they are good movies! But if both people recommended it as bad movies, that still joins the two movies, but it just joins them in a negative aspect

## Makign it a spark problem

* Map input ratings to (userID, (movieID, rating))
* Find every movie piar rated by the same user
    - This can be done with a self-join operation
    - At this point we have (userID, [(movieID1, rating1), ()........ ] )
* Filter out duplicate pairs
* Make the movie pairs the key
    - map to ((movieID1, movieID2...), (rating1, rating2),...)
* groupByKey() to get every rating pair found for each movie pair
* Compute similarity between ratings for each movie in the pair
* Sort, save, and display the results


## Caching RDDs

* In this example, we'll query the final RDD of movie similarities a couple of times
* Any time you will perfform more than one action of an RDD you must cache it!
    - Otherwise Spark might re-evaluate the entire RDD all over again
* Use .cache() or .persist() to do this
    - What is the difference?
    - Persist() optionally lets you cache it to disk instead of just memory, just in a case a node fails at something

 
# Code time!

We start `moviesimilarities.scala`

* When we use `local[*]`, we override anything that is configured. SO if we run it in cluster it won't use cluster.
* We then load movie names iwth the function
    - the `loadMovieNames(): Map[Int, String]`
        + We put the codec UTF8
        + We create a mpa
        + We split the fields
        + And then we add to the map to turn movie IDs to movie names
* Now load the movieRating data 
* Then create a ratings RDD, where
    - For every line we run 
        `data.map(l => l.split("\t")).map(l => (l(0).toInt, (l(1).toInt, l(2).toDouble)))`
    - We have a tuple of a movie ID and rating
* Then we join every movie rated together by same user
    - `ratings.join(ratings)`
* Then we take the current list and filter duplicates
    - `joinedRatings.filter(filterDupliacates)`
        + Where the filterDuplicate function
            * Only returns value when movie1 < movie2
            * This discards other permutations of combinations leaving only the sorted ones
* Now we key everything by moviePairs
    - `uniqueJoinedRatings.map(makePairs)`
        + We want to end with moviePairs themselves
        + the makePairs function:
            * Re-writes the data, 
            * We define some custom scala types
                - `type MovieRating = (Int , Double)`
                - `type UserRatingPair= = (Int, (MovieRating, MovieRating)`
* Then we do groupByKey
    - Collects all the movie ratings for all users out there
* Now we can start measureing similarities
    - `moviePairRatings.mapValues(computeCosineSimilarity).cache()`
    - We are going to call map values to compute cosine similarity 
    - We have function `computeCosineSimilarity`
        + Cosine is only one of the many types of similarities out there
        + In a trigonometry sense, the ratings of two movies are going to diffeernt dimensions, we measure the angle between those
    - We also called cache in that value!
    - We have the entire set of movies viewed together
    - And entire set of similarities
    - We'll use it more than once so we'll cache resultss
* Then we use args[0] which is an input for a movie
    - And we filter for only the movies that contain the movie that we want
* Then we flip things around, so the similarity score comes by the key and then sort by key and take the top 10 to print out

We're gonna run things for real in the command line

We woudl normally use something like SPT, but right now we can export a file!

We have to right click on package and export! We want a java jar file.

Now let's go to the folder and run

```
spark-submit --class io.e_x.spark.MovieSimilarities MovieSims.jar 50
```

Spark-submit is a powerful script - it can integrate with YARN, etc. It figures out how to distribute your application.

**The python script would've taken 15 minutes!!**

p[[p]]







