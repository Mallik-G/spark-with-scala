# Introducing RDDs

RESILIENT DISTRIBUTED DATASET!!!

Abstracts on top of the processing of the data

Encapsulation of very large datasets!

You let the RDD to abstract the computations - allow it to recover from node stuff

The three components are:

* Resilient
    - You allow it to deal with node failures
* Distributed
    - You alloww it to distribute the processing across nodes in cludster
* Dataset   
    - At the end it's just a giant set of data
    - Row after row 
    - Can be key value

# The sparkcontext

* You can create spark contexts
    - In shell you have sparkcontext by default
* This states what the name of the job is, etc
* It's what creates RDDs for you

## Creating RDDs

* val nums = parallelize(List(1,2,3,4))
    - You can just open a set of numbers from a list
* sc.textFile("file://...")
    - or s3n://...., hdfs://....
    - You can run stuff from filesystem
* hiveCtx = HiveContext(sc)
    - rows = hiveCtx.sql("SELECT name, age FROM users")
    - you can integrate with Hive, to query a HDFS directly through the hive tables
* Can also create from
    - JDBC
    - Cassandra
    - HBase
    - ElasticSearch
    - JSON, CSV, sequence files, object files, etc

## Transforming RDDs

You can transform an RDD to create a new RDD, or apply an action to the RDD itself.

* map
* flatmap
* filter
* distinct
* sample
* union, intersection, substract, cartesian

### Example

``` scala
val rdd = sc.parallelize(List(1,2,3,4))
val squares = rdd.map(x => x*x)
```

Many RDD methods accept a function as parameter

``` scala
rdd.map(x => x*x)

OR

def squareIt(x:Int):Int = {
    return x*x
}

rdd.map(squareIt)
```

## RDD actions

Nothing gets computed until you call an action to your RDD

* Collect
    - Takes rdd, and suck it to the script, ie print stuff, etc.
* Count
* CountByValue
* take
* top
* reduce
* ..and more...

## Lazy evaluation

Spark has lazy evaluation to optimize the actualy execution of the things your code does.



