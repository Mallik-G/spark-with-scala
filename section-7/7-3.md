# DataSets in MLLibs

 * They want us to start using this
 * This is where stuff is moving towards
 * But it's not always practical
     - Not everything is an SQL problem
 * Use spark.ml instead of spark.mllib for the preferred DataSet-based API (instead of RDDs)
     - Performs better
     - Will interoperate better with Spark Streaming, Spark SQL, etc
 * Available in Spark 2.0.0+
 * APIs are different

## Let's look at an example

 * We'll do our linear regression example using datasets

### It looks very similar!

We start by importing

``` scala

import org.apache.spark.ml.regression.LinearRegression
import org.apache.spark.sql.types._
import org.apache.spark.ml.linalg.Vectors
```

We start by creating the spark session

``` scala
val spark = SparkSession
                .builder
                .appName("linear") 
                .master("local[*]")
                .getOrCreate()
```

We need to take the sparkContext out of the session, to first create an old RDD, and then give it some commons

``` scala
val inputLines = spark.sparkContext.textFile("../regression.txt")
val data = inputLines.map(_.split(",")).map(x=>(x(0).toDouble, Vectors.dense(x(1).toDouble)))
```

You have to create the vectors.dense

Now we have a `data` RDD - we will convert to DataFrame

``` scala
import spark.implicity._ // to infer schema
val colNames = Seq("label", "features")
val df = data.toDF(colNames: _*) // This just means that we're passing the lsit
```


One way to measure performance of ML model is to use a train/test.

We split half data for building mode and half for testing model.

``` scala  
    // Let's split our data into training data and testing data
    val trainTest = df.randomSplit(Array(0.5, 0.5))
    val trainingDF = trainTest(0)
    val testDF = trainTest(1)
```

Now we create a linear regression model with params:

``` scala

    // Now create our linear regression model
    val lir = new LinearRegression()
      .setRegParam(0.3) // regularization 
      .setElasticNetParam(0.8) // elastic net mixing
      .setMaxIter(100) // max iterations
      .setTol(1E-6) // convergence tolerance
    
```

and fit to our model

``` scala

    // Train the model using our training data
    val model = lir.fit(trainingDF)
    
```

And now we take our test and transform our model using our test data, we add a prediction column to our test dataframe

Now our dataset will contina the prediciotn data and actual data for compre

``` scale

    // Now see if we can predict values in our test data.
    // Generate predictions using our linear regression model for all features in our 
    // test dataframe:
    val fullPredictions = model.transform(testDF).cache()
    
```

Now that we have DFs, we can use sql statement.

``` scala
    // Extract the predictions and the "known" correct labels.
    val predictionAndLabel = fullPredictions.select("prediction", "label").rdd.map(x => (x.getDouble(0), x.getDouble(1))) 
```

And finally print the predictions

``` scala
    // Print out the predicted and actual values for each point
    for (prediction <- predictionAndLabel) {
      println(prediction)
    }
```



