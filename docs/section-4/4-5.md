# Degrees of Separation Spark Code

We start with the `DegreesOfSeparation.scala` file

We make sure to import the `ArrayBuffer` and `LongAccumulator` packages

We select 2 main characters:
* startCharacterID = 5306 // SpiderMan
* targetCharacterID = 14 // ADAM - dunno who but let's see

We now need to create an accumulator counter. In Spark 2+ we can use a AccumulatorLong instead of Accumulator[Int]

``` scala
var hitCounter:Option[LongAccumulator] = None
```

We give it None, as we initialize/create it later wrapping a Long value.

We also declare a custom data type!

``` scala
type BFSData = (Array[Int], Int, String)
type BFSNode = (Int, BFSData)
```

Now we convert our line of input data to our BFSNode object!

``` scala
  /** Converts a line of raw input into a BFSNode */
  def convertToBFS(line: String): BFSNode = {
    
    // Split up the line into fields
    val fields = line.split("\\s+")
    
    // Extract this hero ID from the first field
    val heroID = fields(0).toInt
    
    // Extract subsequent hero ID's into the connections array
    var connections: ArrayBuffer[Int] = ArrayBuffer()
    for ( connection <- 1 to (fields.length - 1)) {
      connections += fields(connection).toInt
    }
    
    // Default distance and color is 9999 and white
    var color:String = "WHITE"
    var distance:Int = 9999
    
    // Unless this is the character we're starting from
    if (heroID == startCharacterID) {
      color = "GRAY"
      distance = 0
    }
    
    return (heroID, (connections.toArray, distance, color))
  }

```

There s a lot of code, we'll cover it

## Main function

We start by initializing the accumulator with zero.

``` scala
hitCounter = Some(sc.longAccumulator("Hit Counter"))
```

This will be available to the entire cluster

Then we call the `createStartingRDD(sc)` function which reads our entire file and converts it into RDD.

The function basically just converts the `convertToBFS` function to all its lines:

``` scala  
  /** Create "iteration 0" of our RDD of BFSNodes */
  def createStartingRdd(sc:SparkContext): RDD[BFSNode] = {
    val inputFile = sc.textFile("../marvel-graph.txt")
    return inputFile.map(convertToBFS)
  }
```

Then we iterate as many as 10 times through the graph. It's an upper bound as the max numeber of iterations.

During the iteration, we create new bfsMaps by calling the function to all the rows of our rdd with a flatMap function:

``` scala
    for (iteration <- 1 to 10) {
      println("Running BFS Iteration# " + iteration)
   
      // Create new vertices as needed to darken or reduce distances in the
      // reduce stage. If we encounter the node we're looking for as a GRAY
      // node, increment our accumulator to signal that we're done.
      val mapped = iterationRdd.flatMap(bfsMap)

        ....rest of code ommitted
```

Let's see what the bfsMap does - it expands a BFSNode into this node and its children!

For each `node:BFSNode`, we extract the data inside first:
``` scala
 // Extract data from the BFSNode
    val characterID:Int = node._1
    val data:BFSData = node._2
    
    val connections:Array[Int] = data._1
    val distance:Int = data._2
    var color:String = data._3
```

It's pretty self explanatory for that part.

We create a `var results:ArrayBuffer[BFSNode] = ArrayBuffer()` to fill up all the children.

If the colour of the current node is gray, we iterate through all the children, and we add to the results a new entry

If we happen to run into the objective node we're looking for, then we increase the hit counter.

And the colour the node as black after iterating through all children

finally preserve the original black node and return the final result.

After the mapping stage, we check if the hit counter is greater than 0, and if it is, then we have found the character, so we just return.

## Reducer

If we haven't found our character, we continue to our reduce stage:

What we do in the reducer is first collect all the data:

``` scala 
    // Extract data that we are combining
    val edges1:Array[Int] = data1._1
    val edges2:Array[Int] = data2._1
    val distance1:Int = data1._2
    val distance2:Int = data2._2
    val color1:String = data1._3
    val color2:String = data2._3


    // Default node values
    var distance:Int = 9999
    var color:String = "WHITE"
    var edges:ArrayBuffer[Int] = ArrayBuffer()
```

We then check if one is the original node with its connections, if so we preserve them

``` scala
    if (edges1.length > 0) {
      edges ++= edges1
    }
    if (edges2.length > 0) {
      edges ++= edges2
    }
```

Then we preserve the minimum distance out of both

``` scala
    if (distance1 < distance) {
      distance = distance1
    }
    if (distance2 < distance) {
      distance = distance2
    }
```

And finally we preserve the darkest colour

``` scala
    if (color1 == "WHITE" && (color2 == "GRAY" || color2 == "BLACK")) {
      color = color2
    }
    if (color1 == "GRAY" && color2 == "BLACK") {
      color = color2
    }
    if (color2 == "WHITE" && (color1 == "GRAY" || color1 == "BLACK")) {
      color = color1
    }
    if (color2 == "GRAY" && color1 == "BLACK") {
      color = color1
    }
```

And then return the edges, distance and colour

``` scala
return (edges.toArray, distance, color)
```






