# Spark Streaming

* Analyses continual streams of data
* Data is aggregated and analyzed at some given interval
* Can take data fed to some port, Amazon Kinesis, HDFS, Kafka, FLume and others
* "Checkpointing" stores state to disk periodically for fault tolerance

## DStreams

* A dstream object breaks up the stream into distinct RDDs
* Simple example
    - val stream = new StramingContext(conf, Seconds(1))
    - val lines = stream.socketTextStream("localhost", 8888)
    - val errors = lines.filter(_.contains("Error"))
    - errors.print()

This listens to log data sent to port 8888, one second at a time, and prints out error lines

You need to kick off job explicitly

* stream.start()
* stream.awaitTermination()

## Windows

* Remember your rdds only contain little chunk of incoming data
* Windowed operations can combine results from multiple batches over some sliding time window
    - window(), reduceByWindow(), reduceByKeyWindow()
* updateStateByKey()
    - Let's you maintian state across many batches as time goes on
    - For example, running counts of some event

## Let's stream

* We'll run a spark streaming script that monitors live tweets from twitter and keeps track of the most popular hashtags as tweets are received.

# CODE

## Step one

* Get a twitter stream and extract just the messages themselves
    - val tweets = TwitterUtils.createStream(ssc, None)
    - val statuses = tweets.map(status => status.getText())

## Step two

Then we're gonna split it into words, we create a dstream that has every word

* val tweetwords = statuses.flatMap(tweetText => tweetText.split(" "))

## Step three

* Eliminate anything that is not a hashtag
* We use the filter() function
* val hashtags = tweetwords.filter(word => word.startsWith("#"))

## Step four

* Convert our RDD of hashtags to kev/val
* val hashtagKeyVals = hashtag.map(hashtag => (hashtag, 1))

## Step five

* Count the results over a sliding window
* A reduce operation adds up all the values for a given key
    - Here, a key is a unique hashtag, and each value is 1
    - So by adding up all the 1s associated with every instance of a hashtag we get the count of that hashtag
* reduceByKeyAndWindow performs this reduce operation over a given window and slide interval
* val hashtagCounts = hashtagKeyValues.reduceByKeyAndWindow((x,y) => x + y, (x,y) => x * y, Seconds(300), Seconds(1))

## Step six

* We sort and output results
* The counts are in the second values, so sort by 2nd value
* val sortedResults = hashtagCounts.transform(rdd => rdd.sortBy(x => x_2, false))
* sortedResults.print



