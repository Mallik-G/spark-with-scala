# Processing Tweets with Spark Streaming

Create a twitter app access token, and copy those to a text file in your product folder.

Go to scala course, click on properties, then java build path, and add the jar in course materials with 3 twitter JARs. Just import them.

Now import the file `PopularHashtags.scala`

# Code

The only thing that we are importing that is new is 

``` scala 
import org.apache.spark.streaming.twitter._
```

Now we setup the logging to just show error:

``` scala
    /** Makes sure only ERROR messages get logged to avoid log spam. */
  def setupLogging() = {
    import org.apache.log4j.{Level, Logger}   
    val rootLogger = Logger.getRootLogger()
    rootLogger.setLevel(Level.ERROR)   
  }
```

Now we configure twitter by importing the source from the files, and adding the fields into the System.setProperty

``` scala
  /** Configures Twitter service credentials using twiter.txt in the main workspace directory */
  def setupTwitter() = {
    import scala.io.Source
    
    for (line <- Source.fromFile("../twitter.txt").getLines) {
      val fields = line.split(" ")
      if (fields.length == 2) {
        System.setProperty("twitter4j.oauth." + fields(0), fields(1))
      }
    }
  }
```

Once we set pu credentials.

We create a streaming context locally, specifying that we'll process every 1 second

``` scala
val ssc = new StreamingContext("local[*]", "PopularHashtags", Seconds(1))
```

We create a receive the a stream of tweets

``` scala
val tweets = TwitterUtils.createStream(ssc, None)
```

We extract the text that comes with the tweet, and just keep the content

``` scala
val statuses = tweets.map(status => status.getText())
```

We're operating on DStreams on top of an RDD... We are not dealing with individual RDDs, but with the stream as a whoel

Then we use flatmap to convert to list of words

``` scala
val tweetwords = statuses.flatMap(tweetText => tweetText.split(" "))
```

**We now have a dstream of strings**

Now we're gonna filter out anything that doesn't start with a hashtag function.

``` scala
val hashtags = tweetwords.filter(word => word.startsWith("#"))
```

And transform that into a key value pair where the value is 1, to do a reduce operation

``` scala
val hashtagKeyValues = hashtags.map(hashtag => (hashtag, 1))
```

We are saying here that the meat of spark streaming...

We want to combine together all of the tweets in RDDs within the window of time that in this case it is 5 minute window. 

And we're going to slide the window every second

``` scala
val hashtagCounts = hashtagKeyValues.reduceByKeyAndWindow( (x,y) => x + y, (x,y) => x - y, Seconds(300), Seconds(1))
```

We have a pretty standard redution function that adds the values. It also tells it how to remove something, which in this case it is x-y. With sliding window we need to take stuff out so that's why its efficienc

**You can also see it written as follows:**

``` scala
val hashtagCounts = hashtagKeyValues.reduceByKeyAndWindow( _ + _, _ -_, Seconds(300), Seconds(1))
```

Now we put the hashtags upfront for sorting and print the results

``` scala
    // Sort the results by the count values
    val sortedResults = hashtagCounts.transform(rdd => rdd.sortBy(x => x._2, false))
    
    // Print the top 10
    sortedResults.print
```

Finally we need to submit a checkpoint just in case it stops, and then start it.

``` scala
    // Set a checkpoint directory, and kick it all off
    // I could watch this all day!
    ssc.checkpoint("C:/checkpoint/")
    ssc.start()
    ssc.awaitTermination()
```












