# Using DataSets instead of RDDs

We are going to take the popular movies file but this time we're going to see how it would look like with dataset

The first part, we still don't change it. We have stuff that loads movies, etc, etc.

``` scala
  /** Load up a Map of movie IDs to movie names. */
  def loadMovieNames() : Map[Int, String] = {
    
    // Handle character encoding issues:
    implicit val codec = Codec("UTF-8")
    codec.onMalformedInput(CodingErrorAction.REPLACE)
    codec.onUnmappableCharacter(CodingErrorAction.REPLACE)

    // Create a Map of Ints to Strings, and populate it from u.item.
    var movieNames:Map[Int, String] = Map()
    
     val lines = Source.fromFile("../ml-100k/u.item").getLines()
     for (line <- lines) {
       var fields = line.split('|')
       if (fields.length > 1) {
        movieNames += (fields(0).toInt -> fields(1))
       }
     }
    
     return movieNames
  }
```


We also create a Movie type

``` scala
final case class Movie(movieID: Int)
```

All we care is about the movie ID

## Code

So now we creat ea spark session as always:

``` scala
    // Use new SparkSession interface in Spark 2.0
    val spark = SparkSession
      .builder
      .appName("PopularMovies")
      .master("local[*]")
      .config("spark.sql.warehouse.dir", "file:///C:/temp") // Necessary to work around a Windows bug in Spark 2.0.0; omit if you're not on Windows.
      .getOrCreate()
    
```

Once we have the spark session object, we load the u.data and create Movie objects from it.

``` scala
val lines = spark.sparkContext.textFile("../ml-100k/u.data").map(x => Movie(x.split("\t")(1).toInt))
```

We have now put structure into the unstructured dataset. Each movie object contains a single column - the movie id

Now we convert hte movie ID into the dataset of movie objects.

Now we can do SQL-ish things....

We had to jump some hoops where we had to map movie ids to number 1s, but here we can do it fast!

``` scala
val topMovieIDs = movieDS.groupBy("movieID")
                            .count()
                            .orderBy(desc("count"))
                            .cache()
```

So now we can do all of this in a single more readable line!

Finally we can just make it human readable

``` scala
// Grab the top 10
    val top10 = topMovieIDs.take(10)
    
    // Load up the movie ID -> name map
    val names = loadMovieNames()
    
    // Print the results
    println
    for (result <- top10) {
      // result is just a Row at this point; we need to cast it back.
      // Each row has movieID, count as above.
      println (names(result(0).asInstanceOf[Int]) + ": " + result(1))
    }

    // Stop the session
    spark.stop()
```

Do remember to stop the session when we're done!




