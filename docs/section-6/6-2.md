# SparkSQL in Action

Let's go through the SparkSQL scala code

We start by importing everything including our sql script

``` scala
import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.spark.sql._
import org.apache.log4j._
```

Now we'll use a dataset instead of a RDD.

We start by creating a spark session

``` scala
val spark = SparkSession
    .builder
    .appName("SparkSQL")
    .master("local[*")
    // just for windows: .config("spark.sql.warehouse.dir", file:///C:/temp)
    .getOrCreate() // if it falls down it can continue
```

Then we need to convert he text file stuff

``` spark
val lines = spark.sparkContext.textFile("../fakefriends.csv")
val people = lines.map(mapper) 
// we still have to convert it to readable format
// But it gives us a dataset
```

We can see the mapper function:

``` scala
case class Person(ID: Int, name: String, age:Int, numFriends:Int)

def mapper(line:String): Person = {
    val friends = line.split(",")
    val person:Person = Person(fields(0).toInt ....)
    return person
}
```
We end up with structure data that then we can create an object with

In order to convert structure RDD to a dataset or inferring schema, we need `import spark.implicits._`

``` scala
import spark.implicits._
val schemaPeople = people.toDS
```

Scala has special optimization methods for DSs

``` scala
schema.printSchema()

schemaPeople.createOrReplaceTempView("people")
```

We created an SQL-like dataset called people

Now we can create a dataframe of results

``` scala
// Now we can run any sql
// we can reference the view we created previously
val teenagers = spark.sql("SELECT * FROM people WHERE age >= 13 AND age <= 19")
```

Then  we just collect the results and print!

We can also see that it prints schema, and how it converts to data structure!

**But there is more than 1 way of doing it!**





