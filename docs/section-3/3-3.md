# Ratings Histogram Walkthrough

We're going to try to explain more about how this works...

We go to run configurations, and click run, which goes through 100k movie ratings and calculates how many scores of each!

## Understand the ratings counter code

First few lines of code:

* Declaring what package this lives in
    - `package mydomain.spark`
* Then import the specific libraries
    - `import org.apache.spark._` 
    - `...SparkContext` 
    - `...log4j` to just print error messages
* Then we create the spark context
    - `val sc = new SparkContext("local[*]", "RatingsCounter") ` 
    - sc is an inmutable spark context object
    - The local[*] means that we can use it locally and with all cores
    - We also give it a name to identify the job

### Load the data

Now we load the data from the actual file...

* `val lines = sc.textFile(".../u.data")` 
* It imports it line by line
* The dataset has UID, MovieID, rating and timestamp

### Extract

Now we transform the data into a new RDD that parses the lines and extracts the line we want

* `val ratings = lines.map(x => x.toString.split("\t")(2))` 
    - Call the lines, split on tabs, and extract number 2

### Perform an action

It's an action so when we call this now spark will go off, compute DAG and chug data

* `val results = ratings.countByValue()` 
* Returns a map, where it reduces the items and counts the number of values

### Sort and display

* `val sortedResults = results.toSeq.sortBy(_._)`
    - Make sure it sorts it by its value


