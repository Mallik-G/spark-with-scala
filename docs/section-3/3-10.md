# Scala/Spark Exercise

## Dataset

* We have a dataset that has columns:
    - CustomerID
    - Item ID
    - Amount spent

We need to add something that adds the amount spent per customer!

## Solution

``` scala
package io.e_x.spark

import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.log4j._

object CustomerSpend {
  
  def parseLine(line: String) = {
    val fields = line.split(",")
    val customerID = fields(0).toInt
    val spent = fields(2).toFloat
    (customerID, spent)
  }
  
  def main(args: Array[String]) = {
    
    Logger.getLogger("org").setLevel(Level.ERROR)
    
    val sc = new SparkContext("local[*]", "CustomerSpend")
    
    val lines = sc.textFile("../customer-orders.csv")
    
    val rdd = lines.map(parseLine)
    
    val totalSpent = rdd.reduceByKey( (x,y) => x + y )
    
    val results = totalSpent.collect() 
    
    results.sorted.foreach(println)    
  }
  
}
```