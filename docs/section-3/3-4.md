# Under the hood of Spark

## An execution plan is created from RDDs

If we take a look at the raings count, what happens when we call countByValue, which is the action?

It figures out an execution plan.

It keeps track for all the things in our RDDs and then constructs a DAG.

iT can be a very simple DAG. It maps the RDD to parse the info that we care about, and then call the count by value action..

That is the execution plan. We can see how it gets manifested, where we have 5 lines of rating data, when we map it, we only get 1, and then we add them all up together.

With map there is a 1 to 1 relationship with input and output. We can keep everything partitioned. Taking a bunch of lines and transforming it - but it can be parallelised

However count by value is not parallelized, but it's called a shuffle operation, which might require to shuffle a lot of data.

## Stages

When optimising spark jobs, we can ask how to minimize shuffle operations.

Once we have execution plan, it looks for places that shuffle data.

Stage 1 can be run together in parallel, which would be the textFile() and map(). 

however stage 2, which is the countByValue has to get all the data for it to be work.

## Tasks

Once we have the stages, it will break them into tasks

The tasks get defined in multiple clusters

It breaks tasks to individual pieces that can eb executed together in parallel. 

## Cluster manager

Finally it just executs it

And then gets the result back.

