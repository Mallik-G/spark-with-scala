# Best Practices for running a cluster

## Specifying memory per executor

* Just use an empty default sparkconf in your driver, this way we'll use the defaults EMR sets up instead, as well as any command line options you pass into spark-submit from your master node
* If executors start failing, you may need to adjust the memory each executor has. For example:
    - spark-submit --executor-memory 1g MovieSimilarities1M.py 260 (from the master node of our cluster)

## Specifying a Cluster Manager

* Can use --master yarn to run a YARN cluster
* EMR sets this up by default

## Running on a cluster

* Get your scripts & data someplace where EMR can access them easily
    - AWS's S3 is a good choice
* Spin up an EMR cluster for Spark using the AWS console
    - Billing starts as soon as it's theer
* Get the external DNS name for the master node, and log into it using the "hadoop" user account and your private key file
* Copy your driver program's JAR file and any files it needs
* Run spark-submit and watch the ountput
* **REMEMBER to terminate the cluster when you're done**













 