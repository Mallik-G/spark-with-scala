# Partitioning: Optimizing for running on Clusters

We had added a new function:

``` scala
val moviePairs = uniqueJoinedRatings.map(makePairs)
    .partitionBy(new HashPartitioner(100))
```

This is actually a very important part

# Optimizing for running on Clusters

* Spark isn't totally magic - you need to think about how your data is partitioned
* Running our movie similarity script as-is might not work at all.
    - That self-join is expensive, and Spark won't distribute it on its own
* Use `.partitionBy()` on an RDD before running on a large operation that benefits from partitioning
    - `join()`, `cogroup()`, `groupWith()`, `join()`, `leftOuterJoin()`, `rightOuterJoin()`, `groupByKey()`, `reduceByKey()`, `combineByKey()`, `lookup()`
    - Those operations will preserve your partitioning in their results too

# Choosing partition size

**Try to normally choose as many partitions as you have cores, or executors that fit within your available memory**

* Too few partitions won't take full advantage of your cluster
* Too many results in too much overhead from shuffling data
* `partitionBy(100)` is usually a reasonable place to start for large operations

You often don't know how big your cluster might be so you need to do educated guesses.

More parallelism isn't always better. Just make sure you don't have any resources idle!

HashPartitioner is the typical partition that we will probably use 





