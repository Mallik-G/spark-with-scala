# Troubleshooting cluster jobs

* It's a dark art
* We have a visual ui that tells us whats going on
* Your master will run a console on port 4040
    - But in EMR it's next to impossible to actually connect to it from outside
    - If you have your own cluster running on your own network, life's a little easier in that respect
    - Let's take a look on our local machine thought

# Taking Tour on Spark UI

We'll run the code locally.

We'll conect to port 4040 localhost.

You can see the jobs currently running, you can see the DAG that was created ,and how it's split up, etc.

You can go into individual stages as well.

Stages represent points where spark needs to shuffle data. There might be opportunities to partition data and avoid shuffling.

Environment tab gives you troubleshooting - right scala/java version? Etc. This would help when things work well in cluster but not locally. You can also see the path for java. More about confi of java.

Finally the executor tab tells you how many executors there are.

If you are running it in a cluster and you have only 1 executor, then there might be a problem.

When job is completed, the webserver gets shut down. Clicking around allows you to play around.


# Troubleshooting cluster jobs

* Logs
    - In standalone mode, they are in the web ui
    - In YARN though the logs are distributed. You need to collect them after the fact using yarn logs -applicationID <APPID>
* While your driver script runs, it will log errors like executors failing to issue heartbeats
    - This generally means you are asking too much of each executor
    - You may need more of them - ie more machines in your cluster
    - Each executor may need more memory
    - Or use partitionBy() to demand less work from individual executors by using smaller partitions.

The most common thing is that you migth not have enough capacity. You 

# Managing dependencies

* Remember your executors aren't necessarily on the same box as your driver script
* Use broadcaster variables to share data outside of RDDs
* Need some java or scala package that is not preloaded on the EMR?
    - Bundle them into your JAR with sbt assembly
    - Or use -jars with spark-submit to add individual libraries taht are on the master
    - Try to just avoid using obscure packages you don't need in the first place. Time is money in your cluster and you're better off not fiddling with it.



