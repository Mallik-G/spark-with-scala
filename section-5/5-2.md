# Elastic Map Reduce

## Distributed Spark

The same jobs that have ran in our PC, we can run in a cluster

Spark figures out which job manager is working underneath and works with them so they can distribute everything

This can be mesos, spark, etc

Some of the elements are:

* Spark Driver
    - Running on master node
    - Communicates with manager to distribute the work
* Cluster Manager
    - Distributes the work 
    - Deals with failures
    - TAlks back when tis done
* Cluster Worker

## Other spark submit params

Many of these will be preconfigured automatically. In elastic MR stuff will be setup automatically.

* --master
    - yarn 
    - hostname:port
    - mesos://masternode:port
    - A master in your SparkConf (or in script) will override this!!
* --num-executors
    - How many executor nodes
    - Must set explicitly with YARN
    - Only 2 by default
    - Might be set by default by environment
* --executor-memory
    - Make sure you don't try to use more memory than you have
    - In the cloud executors might have less memory than you think
* --total-executor-cores


## Elastic Map Reduce

* A quick way to create a cluster with Spark, Hadoop and YARN pre-installed
* You pay by the hour instance and for the network and storage IO
* Let's run our one-million ratings movie recommender on a cluster

**Make sure you turn off any clusters!**

### More about MR

* Very quick and easy way to rent time on a cluster of your own
* Sets up a default spark configuration for you on top of hadoop's yarn cluster manager
* Buzzword alert! We're using hadoop! Well, a part of it anyhow
* Spark also has a built-in standalone cluster manager, and scripts to set its own EC2 based cluster
    - But the AWS console is even easier
* Spark on EMR isn't really expensive, but it's not cheap either
    - Unlike MapReduce with MRJob, you'll be using m3.xlarge instances
    - He racked upabout $30 running spark jobs over a few hours prepping the course
    - You also have to remember to shut down your clusters when you're done or else...
* Make sure things run locally on a subset of your data first



