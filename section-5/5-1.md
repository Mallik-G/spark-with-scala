# Packaging with SBT

A technology that use to package up all dependencies in one jar file

* It's like Maven for Scala
* Manages your library dependencies free for you
    - If you're trying to use the cassandra db 
    - It will know the tree of dependencies that the package depends on
    - You need to make sure that cassandra + its dependencies are installed
    - It gets quite hard
* Can package up all your dependencies in a self-contained jar
* Get it from scala-sbt.org

## Using SBT

* Requires specific structure
* You need
    - Project folder and src next to each other
        + Inside src you have main
            * Inside main you have Scala
* You also need
    - assembly.sbt
* You need to have a build.sbt file
    - name := "Popular movies"
    - version := "1.0"
    - organisation := "io.e_x"
    - scalaVersion := "2.10.6"
    - libraryDependencies ++= Seq(
        + "org.apache.spark" %% "spark-core" %
        + "1.6.1" % "provided"

## Adding dependencies

* Say for example you need to depend on Kafka, which isn't built into spakr
    - You could add
        + "rg.apache.spark" %% "spark-streaming-kafka" % "1.6.1"
    - To your librabry of depdendencies and it will automatically fetch it
* Make sure you use the correct SPark version number, and note that I did not use "provided" on the line

## Building up

* Just run
    - sbt assembly
        + from root folder
* You'll find the JAR in target/scala-2.10 or whatever scala youre building it

* This JAR is self-contained, just use
    - spark-submit <jar file>
* And it will run!

# Let's try it out!

We should download the scala-sbt stuff

Our objective is to package 1m ratings version. That is the end game of our course. We want to run this in a cluster.

If we go to ScalaID, and import `MovieSimilarities1m`

We are now uploading frmo s3: 

``` scala
val data = sc.textFile("s3n://sundog-spark/ml-1m/ratings.dat")
```

If we are in a hadoop cluster it would be a hdfs link instead.

This time we are not specifying `local[*]` as we want whatever is pre-configured in the cluster.

Now let's package it up using sbt.

We can see that the folder already has a sbt, where the `build.sbt` has the following

```
name := "MovieSimilarities1M"

version := "1.0"

organization := "com.sundogsoftware"

scalaVersion := "2.11.8"

libraryDependencies ++= Seq(
"org.apache.spark" %% "spark-core" % "2.0.0" % "provided"
)
```

In the project folder there is also an `assembly.sbt` file:

```
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.3")
```

Now let's create the assmebly!

```
sbt assembly
```

Now it's downloading everything, figuring out all dependencies, and ultimately it will create a JAR file.

If we go back to our SBT folder, we can see a scala-1.10 folder, and we can see the final JAR file.

This is the JAR that we need to distribute to our cluster. We'll do that next!




